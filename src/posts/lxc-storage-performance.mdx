---
title: "Post-Mortem: 2x Storage Performance Loss from LXC"
description: "One Kusama validator was running 2x slower due to Proxmox LXC directory storage using ext4 over ext4. Moving to should LVM fix it."
date: "2025-10-27"
tags: ["infrastructure", "performance", "validators", "lxc", "storage", "polkadot"]
draft: false
---

*tl;dr: rotko.net/ksm02 validator was running 2x slower than identical validators for random write
operations. root cause: directory-backed lxc storage creating ext4-over-ext4 layers. moving to lvm
should resolve the issue.*

---

## the problem

we configured rotko.net/ksm02 with directory-backed storage. this creates an architecture that's
convenient for setup but disastrous for fsync-heavy workloads:

```
Container filesystem (ext4)
    ↓
Loop device (/dev/loop0)
    ↓
.raw disk image file
    ↓
Host filesystem (ext4)
    ↓
Physical disk (NVMe)
```

every write passes through two complete filesystem layers. the kernel translates block I/O → file
I/O → block I/O. Each fsync triggers metadata updates at multiple levels. The container can't use
O_DIRECT effectively through these abstraction layers.

validators sync 1000s of blocks/sec with frequent fsyncs - this latency directly impacts
finalization times and attestation deadlines.

For our validator running RocksDB, this was measured in real latency:
- **Directory storage (ext4/ext4)**: 40.8ms average fsync, ~50 IOPS
- **LVM logical volume**: 18.9ms average fsync, ~108 IOPS

That's **2.15x slower** on identical hardware.

## confirming the issue

Simple fio benchmark simulating RocksDB workload:

| Storage Type | Fsync Latency | IOPS |
|--------------|---------------|------|
| Directory (ext4/ext4) | 40.8ms avg | ~50 |
| LVM | 18.9ms avg | ~108 |

**2.15x performance difference** on identical hardware.

## the fix: migrate to lvm

LVM provides direct block device access - one filesystem layer instead of two:

### migration process

```bash
pvcreate /dev/nvme1n1

vgcreate vg0 /dev/nvme1n1

lvcreate -L 1.6T -n thinpool vg0
lvconvert --type thin-pool vg0/thinpool

pvesm add lvmthin lvmct \
  --vgname vg0 \
  --thinpool thinpool \
  --content rootdir,images
```

## results

after migrating ksm02 to LVM:
- fsync latency: 40.8ms → 18.9ms (2.15x improvement)
- IOPS: ~50 → ~108 (2x improvement)
- Stable performance, no more 100ms+ spikes

## lessons learned

**directory storage works better with zfs hosts**: zfs doesn't double-journal like ext4-over-ext4,
but has its own cow overhead. for ext4 hosts: use lvm.

**only one validator affected**: this was specific to ksm02 which was provisioned with
directory storage. our other validators were already on lvm and didn't have the issue.

**check your storage backend**: If you're running validators in LXC containers, verify your storage
configuration:

```bash
pct config <VMID> | grep rootfs
```

if it shows a path like `/data/images/...`, you're on directory storage and should migrate to LVM
for database workloads.

---

**Configuration**: rotko.net/ksm02 at bkk03, Proxmox VE 8.x, Debian 12, NVMe SSD
**Performance gain**: 2.15x faster fsync (40.8ms → 18.9ms)
**Migration effort**: 3 hours (lvm setup, backup/restore, performance testing, monitoring validation)
**Downtime**: None (backup validator handled duties during migration on 5950X@bkk13)
